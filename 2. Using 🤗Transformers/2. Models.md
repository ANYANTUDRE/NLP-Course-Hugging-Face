(not finished yet)
# Models
In this section we’ll take a closer look at creating and using a model using **TFAutoModel** class.

**TFAutoModel class:** simple but clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.

However, if you know the type of model you want to use, you can use the class that defines its architecture directly. 
Let’s take a look at how this works with a BERT model.

## Creating a Transformer
The first thing we’ll need to do to initialize a **BERT model** is load a configuration object:

```python
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```

The configuration contains many attributes that are used to build the model:
```
print(config)
```
- **hidden_size:** size of the hidden_states vector,
- **num_hidden_layers:** number of layers the Transformer model has.


### Different loading methods
Creating a model from the default configuration initializes it with random values:

```python
from transformers import BertConfig, TFBertModel
config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in Chapter 1, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. 
To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained.

Loading a Transformer model that is already trained is simple — we can do this using the **from_pretrained()** method:
```python
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

As you saw earlier, we could replace **TFBertModel** with the equivalent **TFAutoModel** class. We’ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another (even if the architecture is different)

In the code sample above we didn’t use **BertConfig**, and instead loaded a pretrained model via the **bert-base-cased** identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its model card.

This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.


### Saving methods

Saving a model is as easy as loading one — we use the save_pretrained() method, which is analogous to the **from_pretrained()** method:
```python
model.save_pretrained("directory_on_my_computer")
```
This saves two files to your disk:
```
```

- **config.json** file: metadata and attributes necessary to build the model architecture.
- **tf_model.h5** file: known as the state dictionary; it contains all your model’s weights.

**These two files go hand in hand; the configuration is necessary to know your model’s architecture, while the model weights are your model’s parameters.**


## Using a Transformer model for inference
Transformer models can only process numbers — numbers that the tokenizer generates.
Tokenizers can take care of casting the inputs to the appropriate framework’s tensors, but to help you understand what’s going on, we’ll take a quick look at what must be done before sending the inputs to the model.

Let’s say we have a couple of sequences:
```
sequences = ["Hello!", "Cool.", "Nice!"]
```
The tokenizer converts these to vocabulary indices which are typically called **input IDs**. Each sequence is now a list of numbers!
```
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```
This is a list of encoded sequences: **a list of lists**. Tensors only accept rectangular shapes (think matrices). This “array” is already of rectangular shape, so converting it to a tensor is easy:

```
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```

```
```


### Using the tensors as inputs to the model

Making use of the tensors with the model is extremely simple — we just call the model with the inputs:

```
output = model(model_inputs)
```

While the model accepts a lot of different arguments, only the input IDs are necessary. We’ll explain what the other arguments do and when they are required later, but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.
