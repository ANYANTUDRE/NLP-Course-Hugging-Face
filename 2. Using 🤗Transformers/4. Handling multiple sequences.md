# Handling multiple sequences
In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:

- How do we handle multiple sequences?
- How do we handle multiple sequences of different lengths?
- Are vocabulary indices the only inputs that allow a model to work well?
- Is there such a thing as too long a sequence?
  
Letâ€™s see what kinds of problems these questions pose, and how we can solve them using the ðŸ¤— Transformers API.


## Models expect a batch of inputs

In the previous exercise you saw how sequences get translated into lists of numbers. Letâ€™s convert this list of numbers to a tensor and send it to the model:

```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)

# This line will fail.
model(input_ids)
```

```
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```

Oh no! Why did this fail? â€œWe followed the steps from the pipeline in section 2.

The problem is that we sent a single sequence to the model, whereas ðŸ¤— Transformers models expect multiple sentences by default. 
Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence. 
But if you look closely, youâ€™ll see that the tokenizer didnâ€™t just convert the list of input IDs into a tensor, it added a dimension on top of it:

```python
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```

Letâ€™s try again and add a new dimension:

```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
We print the input IDs as well as the resulting logits â€” hereâ€™s the output:

```python
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```

**Batching** is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:

```python
batched_ids = [ids, ids]
```
This is a batch of two identical sequences!


Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. Thereâ€™s a second issue, though. When youâ€™re trying to batch together two (or more) sentences, they might be of different lengths. If youâ€™ve ever worked with tensors before, you know that they need to be of rectangular shape, so you wonâ€™t be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually pad the inputs.
