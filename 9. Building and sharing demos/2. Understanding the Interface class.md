# Understanding the Interface class

## How to create an Interface
You‚Äôll notice that the Interface class has 3 required parameters: **`Interface(fn, inputs, outputs, ...)`**.

These parameters are:
- **fn:** the prediction function that is wrapped by the Gradio interface. This function can take one or more parameters and return one or more values
- **inputs:** the input component type(s). Gradio provides many pre-built components such as "image" or "mic".
- **outputs:** the output component type(s). Again, Gradio provides many pre-built components e.g. "image" or "label".

For a complete list of components, see the Gradio docs . Each pre-built component can be customized by instantiating the class corresponding to the component.

## A simple example with audio
Gradio provides many different inputs and outputs.
In this example, we‚Äôll build an audio-to-audio function that takes an audio file and simply reverses it.

- **input: the Audio component.** You can specify whether you want the source of the audio to be **a file** that the user uploads or **a microphone** that the user records their voice with.
In addition, we‚Äôll set the "type" to be "numpy", which passes the input data as a tuple of (sample_rate, data) into our function to easily ‚Äúreverse‚Äù it.

- **output: the Audio component** which can automatically render a tuple with a sample rate and numpy array of data as a playable audio file. In this case, we do not need to do any customization, so we will use the string shortcut "audio".

```python
import numpy as np
import gradio as gr

def reverse_audio(audio):
  sr, data = audio
  reversed_audio = (sr, np.flipud(data))
  return reversed_audio

mic = gr.Audio(sources="microphone", type="numpy", label="Speak here...")
gr.Interface(reverse_audio, mic, "audio").launch()
```

## Handling multiple inputs and outputs
Let‚Äôs say we had a more complicated function, with multiple inputs and outputs. 
In the example below, we have a function that takes a **dropdown index, a slider value, and number**, and returns an audio sample of a musical tone.

The key here is that when you pass:
- a list of **input components**, each component corresponds to a parameter in order.
- a list of **output coponents**, each component corresponds to a returned value.


```python
import gradio as gr
import numpy as np

notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]

def generate_tone(note, octave, duration):
    sr = 48000
    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)
    frequency = a4_freq * 2 ** (tones_from_a4 / 12)
    duration = int(duration)
    audio = np.linspace(0, duration, duration * sr)
    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)
    return (sr, audio)

gr.Interface(
    fn=generate_tone,
    inputs=[ gr.Dropdown(notes, type="index),
             gr.Slider(minimum=4, maximum=6, step=1),
             gr.Textbox( type="text", # type="number"  correction applied
                         value=1, label="Duration in seconds"),
            ],
    outputs="audio",
).lauch()
```

### The launch() method

By default, the launch() method will launch the demo in a web server that is running locally. If you are running your code in a Jupyter or Colab notebook, then Gradio will embed the demo GUI in the notebook so you can easily use it.

You can customize the behavior of launch() through different parameters:

- **inline** - whether to display the interface inline on Python notebooks.
- **inbrowser** - whether to automatically launch the interface in a new tab on the default browser.
- **share** - whether to create a publicly shareable link from your computer for the interface. Kind of like a Google Drive link!

### ‚úèÔ∏è Let‚Äôs apply it!

Let‚Äôs build an interface that allows you to demo a **speech-recognition** model. To make it interesting, we will accept either a mic input or an uploaded file.

As usual, we‚Äôll:
- **load our speech recognition model** using the pipeline() function from ü§ó Transformers,
- **implement a transcribe_audio() function** that processes the audio and returns the transcription,
- **wrap this function in an Interface** with the Audio components for the inputs and just text for the output.

```python
from transformers import pipeline
import gradio

model = pipeline("automatic-speech-recognition)

def transcribe_audio(mic=None, file=None):
  if mic is not None:
    audio = mic
  elif file is not None:
    audio = file
  else: 
    return "You must either provide a mic recording or a file"

  transcription = model(audio)["text"]
  return transcription

gr.Interface(
    fn=transcribe_audio,
    """inputs= [
        gr.Audio(source="microphone", type="filepath", optional=True),
        gr.Audio(source="upload", type="filepath", optional=True),
    ],""
    inputs= gr.Audio(sources=["microphone", "upload"], type="filepath"),  # correction applied
    outputs="text",
).launch()
```
That‚Äôs it!
Keep going to see how to share your interface with others!
